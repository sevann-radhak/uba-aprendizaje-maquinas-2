version: '3.7'
services:
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"

  postgres-mlflow:
    image: postgres:13
    container_name: postgres_mlflow
    environment:
      POSTGRES_USER: mlflow
      POSTGRES_PASSWORD: mlflow
      POSTGRES_DB: mlflow_db
    ports:
      - "5433:5432"
    networks:
      - airflow_mlflow_net

  webserver:
    image: apache/airflow:2.8.1
    build: 
      context: .
      dockerfile: ./dockerfiles/Airflow/Dockerfile
    restart: always
    depends_on:
      - postgres
      - s3
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=airflow
      - _AIRFLOW_WWW_USER_PASSWORD=airflow
      - AIRFLOW__WEBSERVER__SECRET_KEY=my_secret_key
      - AWS_ACCESS_KEY_ID=minio
      - AWS_SECRET_ACCESS_KEY=minio123
      - AWS_ENDPOINT_URL=http://s3:9000
    volumes:
      - ./dags:/opt/airflow/dags
    ports:
      - "8080:8080"
    command: bash -c "airflow db init && airflow webserver"
    healthcheck:
      test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3

  scheduler:
    build: 
      context: .
      dockerfile: ./dockerfiles/Airflow/Dockerfile
    restart: always
    depends_on:
      - webserver
    environment:
      - LOAD_EX=n
      - EXECUTOR=Local
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__SECRET_KEY=my_secret_key
    volumes:
      - ./dags:/opt/airflow/dags
    image: apache/airflow:2.8.1
    command: bash -c "airflow db init && airflow scheduler"
    healthcheck:
      test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-scheduler.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3

  s3:
    image: minio/minio:latest
    ports:
      - "9000:9000"
    environment:
      - MINIO_ACCESS_KEY=minio
      - MINIO_SECRET_KEY=minio123
    command: server /data

  create_s3_buckets:
    image: minio/mc:latest
    depends_on:
      - s3
    entrypoint: >
      /bin/sh -c '
      sleep 5;
      /usr/bin/mc config host add s3 http://s3:9000 minio minio123 --api S3v4;
      [[ ! -z "`/usr/bin/mc ls s3 | grep challenge`" ]] || /usr/bin/mc mb s3/mlflow;
      /usr/bin/mc policy download s3/mlflow;
      [[ ! -z "`/usr/bin/mc ls s3 | grep challenge`" ]] || /usr/bin/mc mb s3/data;
      /usr/bin/mc policy download s3/data;
      exit 0;
      '

  mlflow:
    build:
      context: .
      dockerfile: dockerfiles/mlflow/Dockerfile
    container_name: mlflow
    depends_on:
      - postgres-mlflow
    environment:
      MLFLOW_TRACKING_URI: postgresql+psycopg2://mlflow:mlflow@postgres-mlflow:5432/mlflow_db
    ports:
      - "5000:5000"
    networks:
      - airflow_mlflow_net
    command: mlflow server --backend-store-uri postgresql+psycopg2://mlflow:mlflow@postgres-mlflow:5432/mlflow_db --default-artifact-root /mlflow/artifacts --host 0.0.0.0


networks:
  airflow_mlflow_net:
    driver: bridge